<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>moog_demos.human_agent API documentation</title>
<meta name="description" content="Human gui to demo tasks â€¦">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>moog_demos.human_agent</code></h1>
</header>
<section id="section-intro">
<p>Human gui to demo tasks.</p>
<p>This contains tools for an interactive gui controlled by a human. The main class
is HumanAgent, which takes and environment and runs it in an iteractive display
that shows the rendered environement, plot of recent rewards, and a
human-playable action space (e.g. cartoon joystick, arrow keys, etc.).</p>
<p>This is useful for testing task prototypes.</p>
<p>Note: When recording gifs, be sure to pressing the 'esc' key when you want to
stop the demo and write the gif.</p>
<p>Note: If the reward plot does not appear in your figure window, that is probably
because your monitor screen is not tall enough to fit it, given the rendering
size you chose. Consider using a smaller render_size in the renderer (eg. 256).</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="moog_demos.human_agent.HumanAgent"><code class="flex name class">
<span>class <span class="ident">HumanAgent</span></span>
<span>(</span><span>env,<br>render_size,<br>fps=10,<br>reward_history=20,<br>observation_image_key='image',<br>gif_writer=None,<br>reward_border_width=10)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HumanAgent():
    &#34;&#34;&#34;Human-playable agent.

    This provides a gui for human interaction with an environment. The gui
    displays the rendered environment, a plot of rewards over recent history,
    and a frame which may contain a joystick, depending on the action space.

    Note: This agent does not abide by the standard RL agent api in that it does
    not have a .step() method taking an observation and returning an action.
    This is because due to Tkinter&#39;s interactive .mainloop() function, the
    entire environment interaction must be implemented as a callback in the gui.
    &#34;&#34;&#34;

    def __init__(self, env, render_size, fps=10, reward_history=20,
                 observation_image_key=&#39;image&#39;, gif_writer=None,
                 reward_border_width=10):
        &#34;&#34;&#34;Constructor.

        Args:
            env: instance of moog.environment.Environment. The environment
                observations are assumed to have a key &#39;image&#39; whose value is an
                image with height and width render_size.
            render_size: Int. Width and height of observation_image_key value of
                the environment observation image.
            fps: Int. Frames per second to run, if possible. Note: The tkinter
                gui interface and matplotlib rendering is slow, often taking
                about 40 milliseconds for 256x256 rendering. Furthermore,
                depending on the environment and physics, stepping the
                environent can take some time (usually no more than 10
                milliseconds). Therefore, the fastest fps this gui can run is
                20fps for 256x256 rendering, and if the given fps is higher than
                this it will be capped. This slowness is mostly due to
                matplotlib and tkinter, not the environment or rendering itself,
                and does not occur when training agents or using mworks for
                psychophysics.
            reward_history: Int. Number of history timesteps to plot the reward
                for.
            observation_image_key: String. Key of the observation that is the
                image.
            gif_writer: Optional instance of a gif writer. This writer should
                have a .close() method and an .add(frame) method.
            reward_border_width: Int. Width of the red/green border to render
                when a positive/negative reward is given.
        &#34;&#34;&#34;
        self._env = env
        self._ms_per_step = 1000. / fps
        self._reward_history = reward_history
        self._observation_image_key = observation_image_key
        self._gif_writer = gif_writer
        self._reward_border_width = reward_border_width

        # This will be used later to capture a section of the screen
        if self._gif_writer:
            self._screen_capture = mss.mss()

        # Create root Tk window and fix its size
        self.root = tk.Tk()
        frame_width = str(render_size)
        frame_height = str(int(_WINDOW_ASPECT_RATIO * render_size))
        self.root.geometry(frame_width + &#39;x&#39; + frame_height)

        # Bind escape key to terminate gif_writer and exit
        def _escape(event):
            if self._gif_writer:
                self._gif_writer.close()
            sys.exit()

        self.root.bind(&#39;&lt;Escape&gt;&#39;, _escape)

        ########################################################################
        # Create the environment display and pack it into the top of the window.
        ########################################################################

        image = env.reset().observation[self._observation_image_key]
        self._env_canvas = tk.Canvas(
            self.root, width=render_size, height=render_size)
        self._env_canvas.pack(side=tk.TOP)
        img = ImageTk.PhotoImage(image=Image.fromarray(image))
        self._env_canvas.img = img
        self.image_on_canvas = self._env_canvas.create_image(
            0, 0, anchor=&#34;nw&#34;, image=self._env_canvas.img)

        ########################################################################
        # Create the gui frame and pack it into the bottom of the window.
        ########################################################################

        canvas_half_width = render_size / 2
        action_space = env.action_space
        if isinstance(action_space, action_spaces.Composite):
            num_agents = len(action_space.action_spaces)
            a_spaces = action_space.action_spaces.values()
            if (num_agents == 2 and all(
                    isinstance(a, action_spaces.Grid) for a in a_spaces)):
                logging.info(
                    &#39;2-player Grid action space. One player uses arrow keys &#39;
                    &#39;and the other uses [a, s, d, w].&#39;)
                # Two-player Grid action spaces
                self.gui_frame = gui_frames.TwoPlayerGridActions(
                    self.root,
                    canvas_half_width=canvas_half_width,
                    player_0=action_space.action_keys[0],
                    player_1=action_space.action_keys[1],
                )
            elif (num_agents == 2 and all(
                    isinstance(a, action_spaces.Joystick) for a in a_spaces)):
                logging.info(
                    &#39;2-player Joystick action space. One player uses arrow keys &#39;
                    &#39;and the other uses [a, s, d, w].&#39;)
                # Two-player Joystick action spaces
                self.gui_frame = gui_frames.TwoPlayerJoystick(
                    self.root,
                    canvas_half_width=canvas_half_width,
                    player_0=action_space.action_keys[0],
                    player_1=action_space.action_keys[1],
                )
            else:
                logging.info(
                    &#39;Composite action space provided, human controls only the &#39;
                    &#39;first agent.&#39;)
                action_space = list(action_space.action_spaces.values())[0]

        if isinstance(action_space, action_spaces.Joystick):
            logging.info(
                &#39;Joystick action space, drag and move the joystick at the &#39;
                &#39;bottom of the window.&#39;)
            self.gui_frame = gui_frames.JoystickFrame(
                self.root,
                canvas_half_width=canvas_half_width,
                motion_zone_radius=canvas_half_width - 5,
            )
        elif isinstance(action_space, action_spaces.Grid):
            logging.info(&#39;Grid action space, use arrow keys.&#39;)
            self.gui_frame = gui_frames.GridActions(
                self.root,
                canvas_half_width=canvas_half_width,
            )
        elif isinstance(action_space, action_spaces.SetPosition):
            logging.info(&#39;SetPosition action space, click on the frame to act.&#39;)
            self.gui_frame = gui_frames.SetPositionFrame(
                self._env_canvas,
                canvas_half_width=canvas_half_width,
            )
        elif not isinstance(action_space, action_spaces.Composite):
            raise ValueError(
                &#39;Cannot demo action space {}&#39;.format(env.action_space))

        if not isinstance(action_space, action_spaces.SetPosition):
            self.gui_frame.canvas.pack(side=tk.BOTTOM)

        ########################################################################
        # Create the reward plot and pack it into the middle of the window.
        ########################################################################

        # This figuresize and the subplot adjustment is hand-crafted to make it
        # look okay for _WINDOW_ASPECT_RATIO 2.7. Ideally, we would infer the
        # space available for this reward plot and size the figure accordingly,
        # but I could not easily figure out how to do that --- it seems like
        # matplotlib doesn&#39;t count axis ticks and labels as part of the figure
        # size so those are cut off without handcrafting some subplot
        # adjustment.
        fig = plt.Figure(figsize=(2, 2), dpi=100)
        fig.subplots_adjust(bottom=0.5, left=0.25, right=0.95)

        self._ax_reward = fig.add_subplot(111)
        self._ax_reward.set_ylabel(&#39;Reward&#39;)
        self._ax_reward.set_xlabel(&#39;Time&#39;)
        self._ax_reward.axhline(y=0.0, color=&#39;lightgrey&#39;)

        # Plot rewards as a bar plot
        self._reset_rewards()
        reward_ticks = np.arange(-1 * self._reward_history + 1, 1)
        self.rewards_plot = self._ax_reward.bar(reward_ticks, self._rewards)

        # Create canvas in which to draw the reward plot and pack it
        # A tk.DrawingArea.
        self.rewards_canvas = backend_tkagg.FigureCanvasTkAgg(
            fig, master=self.root)
        self.rewards_canvas.draw()
        self.rewards_canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH)

        ########################################################################
        # Start run loop, automatically running the environment.
        ########################################################################

        self.root.after(math.floor(self._ms_per_step), self.step)
        self.root.mainloop()

    def _reset_rewards(self):
        self._rewards = np.zeros(self._reward_history)
        self._reward_range = [-1e-3, 1e-3]

    def render(self, observation):
        &#34;&#34;&#34;Render the environment display and reward plot.&#34;&#34;&#34;
        # Put green border if positive reward, red border if negative reward
        observation_image = observation.observation[self._observation_image_key]

        if self._reward_border_width:
            # Add a red/green border to the image for positive/negative reward.
            if sum(self._rewards[-4:]) &gt; 0:
                show_border = True
                border_color = np.array([0, 255, 0], dtype=np.uint8)
            elif sum(self._rewards[-4:]) &lt; 0:
                show_border = True
                border_color = np.array([255, 0, 0], dtype=np.uint8)
            else:
                show_border = False

            if show_border:
                observation_image[:self._reward_border_width] = border_color
                observation_image[-self._reward_border_width:] = border_color
                observation_image[:, :self._reward_border_width] = border_color
                observation_image[:, -self._reward_border_width:] = border_color

        # Set the image in the environment display to the new observation
        self._env_canvas.img = ImageTk.PhotoImage(Image.fromarray(
            observation_image))
        self._env_canvas.itemconfig(
            self.image_on_canvas, image=self._env_canvas.img)

        # Set the reward plot data to the current self._rewards
        for rect, h in zip(self.rewards_plot, self._rewards):
            rect.set_height(h)
            rect.set_facecolor(&#39;g&#39; if h &gt; 0 else &#39;r&#39;)
        self.rewards_canvas.draw()
        self._ax_reward.set_ylim(*self._reward_range)

    def step(self):
        &#34;&#34;&#34;Take an action in the environment and render.&#34;&#34;&#34;
        step_start_time = time.time()
        action = self.gui_frame.action  # action from the gui
        observation = self._env.step(action)

        # Update rewards and reward_range
        self._rewards[:-1] = self._rewards[1:]
        self._rewards[-1] = observation.reward
        if observation.reward:
            self._reward_range[0] = min(
                self._reward_range[0], observation.reward)
            self._reward_range[1] = max(
                self._reward_range[1], observation.reward)

        # display new observation
        self.render(observation)
        if observation.last():
            self._reset_rewards()

        # Screengrab the window and update the gif_writer
        if self._gif_writer:
            window = {
                &#39;top&#39;: self.root.winfo_rooty(),
                &#39;left&#39;: self.root.winfo_rootx(),
                &#39;width&#39;: int(self.root.winfo_width()),
                &#39;height&#39;: int(self.root.winfo_height()),
            }
            img = np.asarray(self._screen_capture.grab(window))
            # For some reason screen capture switches red and blue color
            # channels
            img = img[:, :, [2, 1, 0, 3]]
            self._gif_writer.add(img)

        # Recurse to step again after self._ms_per_step milliseconds
        step_end_time = time.time()
        delay = (step_end_time - step_start_time) * 1000  # convert to ms
        self.root.after(math.floor(self._ms_per_step - delay), self.step)</code></pre>
</details>
<div class="desc"><p>Human-playable agent.</p>
<p>This provides a gui for human interaction with an environment. The gui
displays the rendered environment, a plot of rewards over recent history,
and a frame which may contain a joystick, depending on the action space.</p>
<p>Note: This agent does not abide by the standard RL agent api in that it does
not have a .step() method taking an observation and returning an action.
This is because due to Tkinter's interactive .mainloop() function, the
entire environment interaction must be implemented as a callback in the gui.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong></dt>
<dd>instance of moog.environment.Environment. The environment
observations are assumed to have a key 'image' whose value is an
image with height and width render_size.</dd>
<dt><strong><code>render_size</code></strong></dt>
<dd>Int. Width and height of observation_image_key value of
the environment observation image.</dd>
<dt><strong><code>fps</code></strong></dt>
<dd>Int. Frames per second to run, if possible. Note: The tkinter
gui interface and matplotlib rendering is slow, often taking
about 40 milliseconds for 256x256 rendering. Furthermore,
depending on the environment and physics, stepping the
environent can take some time (usually no more than 10
milliseconds). Therefore, the fastest fps this gui can run is
20fps for 256x256 rendering, and if the given fps is higher than
this it will be capped. This slowness is mostly due to
matplotlib and tkinter, not the environment or rendering itself,
and does not occur when training agents or using mworks for
psychophysics.</dd>
<dt><strong><code>reward_history</code></strong></dt>
<dd>Int. Number of history timesteps to plot the reward
for.</dd>
<dt><strong><code>observation_image_key</code></strong></dt>
<dd>String. Key of the observation that is the
image.</dd>
<dt><strong><code>gif_writer</code></strong></dt>
<dd>Optional instance of a gif writer. This writer should
have a .close() method and an .add(frame) method.</dd>
<dt><strong><code>reward_border_width</code></strong></dt>
<dd>Int. Width of the red/green border to render
when a positive/negative reward is given.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="moog_demos.human_agent.HumanAgent.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self, observation)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self, observation):
    &#34;&#34;&#34;Render the environment display and reward plot.&#34;&#34;&#34;
    # Put green border if positive reward, red border if negative reward
    observation_image = observation.observation[self._observation_image_key]

    if self._reward_border_width:
        # Add a red/green border to the image for positive/negative reward.
        if sum(self._rewards[-4:]) &gt; 0:
            show_border = True
            border_color = np.array([0, 255, 0], dtype=np.uint8)
        elif sum(self._rewards[-4:]) &lt; 0:
            show_border = True
            border_color = np.array([255, 0, 0], dtype=np.uint8)
        else:
            show_border = False

        if show_border:
            observation_image[:self._reward_border_width] = border_color
            observation_image[-self._reward_border_width:] = border_color
            observation_image[:, :self._reward_border_width] = border_color
            observation_image[:, -self._reward_border_width:] = border_color

    # Set the image in the environment display to the new observation
    self._env_canvas.img = ImageTk.PhotoImage(Image.fromarray(
        observation_image))
    self._env_canvas.itemconfig(
        self.image_on_canvas, image=self._env_canvas.img)

    # Set the reward plot data to the current self._rewards
    for rect, h in zip(self.rewards_plot, self._rewards):
        rect.set_height(h)
        rect.set_facecolor(&#39;g&#39; if h &gt; 0 else &#39;r&#39;)
    self.rewards_canvas.draw()
    self._ax_reward.set_ylim(*self._reward_range)</code></pre>
</details>
<div class="desc"><p>Render the environment display and reward plot.</p></div>
</dd>
<dt id="moog_demos.human_agent.HumanAgent.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self):
    &#34;&#34;&#34;Take an action in the environment and render.&#34;&#34;&#34;
    step_start_time = time.time()
    action = self.gui_frame.action  # action from the gui
    observation = self._env.step(action)

    # Update rewards and reward_range
    self._rewards[:-1] = self._rewards[1:]
    self._rewards[-1] = observation.reward
    if observation.reward:
        self._reward_range[0] = min(
            self._reward_range[0], observation.reward)
        self._reward_range[1] = max(
            self._reward_range[1], observation.reward)

    # display new observation
    self.render(observation)
    if observation.last():
        self._reset_rewards()

    # Screengrab the window and update the gif_writer
    if self._gif_writer:
        window = {
            &#39;top&#39;: self.root.winfo_rooty(),
            &#39;left&#39;: self.root.winfo_rootx(),
            &#39;width&#39;: int(self.root.winfo_width()),
            &#39;height&#39;: int(self.root.winfo_height()),
        }
        img = np.asarray(self._screen_capture.grab(window))
        # For some reason screen capture switches red and blue color
        # channels
        img = img[:, :, [2, 1, 0, 3]]
        self._gif_writer.add(img)

    # Recurse to step again after self._ms_per_step milliseconds
    step_end_time = time.time()
    delay = (step_end_time - step_start_time) * 1000  # convert to ms
    self.root.after(math.floor(self._ms_per_step - delay), self.step)</code></pre>
</details>
<div class="desc"><p>Take an action in the environment and render.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="moog_demos" href="index.html">moog_demos</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="moog_demos.human_agent.HumanAgent" href="#moog_demos.human_agent.HumanAgent">HumanAgent</a></code></h4>
<ul class="">
<li><code><a title="moog_demos.human_agent.HumanAgent.render" href="#moog_demos.human_agent.HumanAgent.render">render</a></code></li>
<li><code><a title="moog_demos.human_agent.HumanAgent.step" href="#moog_demos.human_agent.HumanAgent.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
